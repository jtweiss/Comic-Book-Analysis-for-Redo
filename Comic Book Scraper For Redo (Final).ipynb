{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comic Book Scraper For Redo (Final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   At work recently, I had the task of pricing 14 long boxes of comics in two days. Each long box contains approximately 275 comics that my boss wanted cataloged and priced. Since I was the only working on this assignment, I needed a way to quickly and effectively price this collection before the sale. I then realized I could use my skills in Python to make this assignment a lot easier. I found a site, mycomicbookshop.com, that conveniently provides a price and an \"Add to Cart Option\" for most of the comics in the collection. The only way I was able to complete this assignment is to add each comic to the cart and then scrape all the information I would need later, as opposed to manually entering it into Excel. This code is a sample of about 600 of those comics, and I thought it would be interesting to perform an analysis with this information.\n",
    "\n",
    "Some things to note before continuing: \n",
    "\n",
    "1. The owner of this collection of comics is not comfortable with the contents of his entire collection being posted online at this time, so this project will only be on the contents of approximately three long boxes selected at random.\n",
    "2. This version solves the error when the program encounters a 503-gateway error on select pages.\n",
    "3. Simple visualizations will be performed, but the bulk of the EDA will be done in Tableau.\n",
    "4. An explanation for the comic book grading system can be found here, https://www.mycomicshop.com/help/grading, for those unfamiliar with how the grading system works.\n",
    "5. After conversing with some people in the comic collecting community about this project, I have been informed that the prices on mycomicbookshop.com are slightly overpriced. To compensate this we took 25% off the calculated price in addition to the discount we gave on the entire lot.\n",
    "6. I had some help writing this code, and I would like to thank user doctorevil92 for helping me get past the 503 gateway error and helping me out when I struggled with overcoming t. I could not have done this without him and I am extremely grateful for his help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Cleaning from mycomicbookshop.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"myusername\"\n",
    "PASSWORD = \"mypassword\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver  #install selenium\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pprint\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from lxml import html # pip install lxml\n",
    "import csv\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_PAUSE = 1.0 # pause\n",
    "\n",
    "def wait_by_xpath(xp, how_long_to_wait): # xp is string, how_long_to_wait float - the number of seconds to wait\n",
    "    try:\n",
    "        WebDriverWait(driver, how_long_to_wait).until(EC.presence_of_element_located((By.XPATH, xp)) )\n",
    "        time.sleep(TIME_PAUSE)\n",
    "        return 1 # success\n",
    "    except TimeoutException:\n",
    "        print (\"Too much time has passed while waiting for\", xp)\n",
    "        return 0 # fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_string(entry_string): # remove \"\\n\", \"\\t\" and double spaces\n",
    "    exit_string = entry_string.replace(\"\\n\", \"\")\n",
    "    exit_string = exit_string.replace(\"\\t\", \"\")\n",
    "    exit_string = exit_string.replace(\"\\r\", \"\")\n",
    "    while \"  \" in exit_string:\n",
    "        exit_string = exit_string.replace(\"  \", \" \")\n",
    "    if len(exit_string) > 0: # remove first space\n",
    "        if exit_string[0] == ' ':\n",
    "            exit_string = exit_string[1:len(exit_string)]\n",
    "    if len(exit_string) > 0: # remove last space\n",
    "        if exit_string[len(exit_string)-1] == ' ':\n",
    "            exit_string = exit_string[0:len(exit_string)-1]\n",
    "\n",
    "    return exit_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PARSE(input_url, input_name, input_price, input_html):\n",
    "    data_to_return = {}\n",
    "    data_to_return[\"URL\"] = input_url\n",
    "    data_to_return[\"Full Price\"] = input_price\n",
    "\n",
    "    tree = html.document_fromstring(input_html)\n",
    "\n",
    "    data_to_return[\"Title\"] = ''\n",
    "    data_to_return[\"Series\"] = ''\n",
    "    data_to_return[\"Condition\"] = ''\n",
    "    #title_el = tree.xpath(\"//div[@class='primary']/h1[@class='top']\")\n",
    "    #if len(title_el) != 0:\n",
    "    full_title_text = input_name\n",
    "    data_to_return[\"Title\"] = full_title_text\n",
    "\n",
    "    if \"(\" in full_title_text and \")\" in full_title_text:\n",
    "        if full_title_text.find(\"(\") < full_title_text.find(\")\"):\n",
    "            data_to_return[\"Series\"] = full_title_text[full_title_text.find(\"(\")+1 : full_title_text.find(\")\")]\n",
    "\n",
    "            data_to_return[\"Condition\"] = fix_string(\"\".join(char for char in full_title_text[full_title_text.find(\")\")+1 : len(full_title_text)] if not char.isdigit()))\n",
    "\n",
    "    \n",
    "    data_to_return[\"Month Published\"] = ''\n",
    "    data_to_return[\"Year Published\"] = ''\n",
    "    published_el = tree.xpath(\"//div[@class='primary']//div/text()[contains(., 'Published')]/following-sibling::a[@href][1]\")\n",
    "    if len(published_el) != 0:\n",
    "        published_time_fullstring = fix_string(published_el[0].text_content())\n",
    "        published_time_array = [pub_item for pub_item in published_time_fullstring.split(\" \") if pub_item!='']\n",
    "        if len(published_time_array) == 1:\n",
    "            # probably year only\n",
    "            try:\n",
    "                probe_year = int(published_time_array[0])\n",
    "                data_to_return[\"Year Published\"] = probe_year\n",
    "            except ValueError:\n",
    "                print (\"Unable to parse year of publishing with 1 element on URL\", input_url)\n",
    "                pass\n",
    "\n",
    "        elif len(published_time_array) == 2:\n",
    "            # should be month and year\n",
    "            try:\n",
    "                probe_year = int(published_time_array[1])\n",
    "                data_to_return[\"Year Published\"] = probe_year\n",
    "                data_to_return[\"Month Published\"] = published_time_array[0]\n",
    "            except ValueError:\n",
    "                print (\"Unable to parse year of publishing with 2 elements on URL\", input_url)\n",
    "                pass\n",
    "        else:\n",
    "            print (\"Unknown form of date for\", published_time_array)\n",
    "\n",
    "\n",
    "    data_to_return[\"Publisher\"] = ''\n",
    "    publisher_el = tree.xpath(\"//div[@class='primary']//div/text()[contains(., 'Published')]/following-sibling::text()[contains(., 'by')]/following-sibling::a[@href][1]\")\n",
    "    if len(publisher_el) != 0:\n",
    "        data_to_return[\"Publisher\"] = publisher_el[0].text_content()\n",
    "\n",
    "    data_to_return[\"Tags\"] = ''\n",
    "    tags_els = tree.xpath(\"//div[@class='primary']//div[@class='title']/following-sibling::div/text()[contains(., 'Tags')]/following-sibling::a[@href]\")\n",
    "    for tag_el in tags_els:\n",
    "        data_to_return[\"Tags\"] = data_to_return[\"Tags\"] + tag_el.text_content() + \", \"\n",
    "    data_to_return[\"Tags\"] = data_to_return[\"Tags\"][0:data_to_return[\"Tags\"].rfind(\", \")]\n",
    "\n",
    "    data_to_return[\"Description\"] = ''\n",
    "    descr_el = tree.xpath(\"//div[@class='primary']//div[@class='tabcontents']/div[@class='issuegrades']/following-sibling::p[1]/text()\")\n",
    "    if len(descr_el) != 0:\n",
    "        data_to_return[\"Description\"] = descr_el[0]\n",
    "\n",
    "    data_to_return[\"Cover Price\"] = '' # get from description text\n",
    "    if len(descr_el) != 0:\n",
    "        descr_to_parse = descr_el[0].lower()\n",
    "        cover_price_text = fix_string(descr_to_parse[descr_to_parse.rfind(\"cover price\")+len(\"cover price\") : descr_to_parse.rfind(\".\")])\n",
    "        try:\n",
    "            probe_price = float(cover_price_text.replace(\"$\", \"\"))\n",
    "            data_to_return[\"Cover Price\"] = cover_price_text\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    return data_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first, login\n",
    "try:\n",
    "    driver.get(\"https://www.mycomicshop.com/login\")\n",
    "except:\n",
    "    print (\"Some kind of exception, quit!\")\n",
    "    driver.quit()\n",
    "    sys.exit(0)\n",
    "\n",
    "    \n",
    "try:\n",
    "    username_el = driver.find_element_by_xpath(\"//input[@name='CustomerEmail']\")\n",
    "    username_el.send_keys(USERNAME)\n",
    "    password_el = driver.find_element_by_xpath(\"//input[@name='CustomerPassword']\")\n",
    "    password_el.send_keys(PASSWORD)\n",
    "    login_el = driver.find_element_by_xpath(\"//input[@type='submit' and @value='Log In']\")\n",
    "    driver.execute_script(\"arguments[0].click();\", login_el)\n",
    "    time.sleep(5.0)\n",
    "except:\n",
    "    print (\"Some kind of exception while logging in, quit!\")\n",
    "    driver.quit()\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "try:\n",
    "    driver.get(\"https://www.mycomicshop.com/cart\")\n",
    "    wait_for_cart = wait_by_xpath(\"//h2[text()='Saved For Later']/following-sibling::table[1]/tbody/tr/td[1]/a[contains(@href, 'search?')]/../..\", 30)\n",
    "    if wait_for_cart == 0:\n",
    "        print (\"Failed wait for cart elements, quit!\")\n",
    "        driver.quit()\n",
    "        sys.exit(0)\n",
    "    innerHTML_cart = driver.execute_script(\"return document.body.innerHTML\")\n",
    "    htmlElem_cart = html.document_fromstring(innerHTML_cart)\n",
    "except:\n",
    "    print (\"Some kind of exception while opening cart link, quit!\")\n",
    "    driver.quit()\n",
    "    sys.exit(0)\n",
    "### login done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get a list of items from cart html, visit all urls that need to be visited\n",
    "items_to_visit = [] # keys: name, url, price\n",
    "comic_els = htmlElem_cart.xpath(\"//h2[text()='Saved For Later']/following-sibling::table[1]/tbody/tr/td[1]/a[contains(@href, 'search?')]/../..\")\n",
    "for comic_el in comic_els:\n",
    "    comic_link_el = comic_el.xpath(\"./td[1]/a[@href]\")\n",
    "    comic_price_el = comic_el.xpath(\"./td[2]\")\n",
    "\n",
    "    if len(comic_link_el) != 0 and len(comic_price_el) != 0:\n",
    "        items_to_visit.append( {\"name\":comic_link_el[0].text_content(), \"price\":fix_string(comic_price_el[0].text_content()), \"url\": \"https://www.mycomicshop.com\" + comic_link_el[0].attrib[\"href\"]} )\n",
    "\n",
    "# create a database, get already scraped links\n",
    "db_conn = sqlite3.connect(\"COMICS_DB.db\")\n",
    "db_cursor = db_conn.cursor()\n",
    "db_cursor.execute(\"CREATE TABLE IF NOT EXISTS ComicsData (url TEXT, name TEXT, price TEXT, html TEXT)\")\n",
    "\n",
    "already_scraped_links = {}\n",
    "for already_scraped_item in db_cursor.execute(\"SELECT url FROM ComicsData\"):\n",
    "    already_scraped_links[already_scraped_item[0]] = ''\n",
    "\n",
    "# now visit every comic url and save html if some kind of verificator is found.\n",
    "for item_to_visit in items_to_visit:\n",
    "    if item_to_visit[\"url\"] in already_scraped_links:\n",
    "        continue # this was already scraped\n",
    "\n",
    "    # if here, try to open link\n",
    "    try:\n",
    "        driver.get(item_to_visit[\"url\"])\n",
    "        wait_for_verificator = wait_by_xpath(\"//div[@class='primary']/h1[@class='top']\", 20)\n",
    "        if wait_for_verificator == 0:\n",
    "            continue\n",
    "        innerHTML_cart = driver.execute_script(\"return document.body.innerHTML\")\n",
    "        htmlElem_cart = html.document_fromstring(innerHTML_cart)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"Manual interrupt, quit everything!\")\n",
    "        driver.quit()\n",
    "        db_cursor.close()\n",
    "        db_conn.close()\n",
    "        sys.exit(0)\n",
    "\n",
    "    except:\n",
    "        time.sleep(2.0)\n",
    "        continue\n",
    "\n",
    "    #if still here, save html, update scraped links and continue to the next one.\n",
    "    db_cursor.execute(\"INSERT INTO ComicsData (url, name, price, html) VALUES (?,?,?,?)\",\n",
    "                  (item_to_visit[\"url\"], item_to_visit[\"name\"], item_to_visit[\"price\"], innerHTML_cart) ) # not necessary to insert all columns!\n",
    "    db_conn.commit()\n",
    "    already_scraped_links[item_to_visit[\"url\"]] = ''\n",
    "    print (\"Scraped link number\", items_to_visit.index(item_to_visit)+1, \"/\", len(items_to_visit))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse for all items in items_to_visit!\n",
    "print (\"Parsing listed comics...\")\n",
    "headers_in_order = ['Title', 'Series', 'Condition', 'Publisher', 'Month Published', 'Year Published', 'Tags', 'Description', 'Cover Price', 'Full Price', 'URL']\n",
    "output_filename = datetime.now().strftime(\"%m-%d-%Y %H_%M_%S\") + \".csv\"\n",
    "with open(output_filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfil:\n",
    "    writer = csv.writer(csvfil, delimiter=\";\", quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(headers_in_order) # write headers\n",
    "    \n",
    "    for item_to_fetch in items_to_visit:\n",
    "        # for each link, get data from database\n",
    "        for fetched_data in db_cursor.execute(\"SELECT * FROM ComicsData WHERE url=?\", (item_to_fetch[\"url\"],)):\n",
    "            parsed_data = PARSE(fetched_data[0], fetched_data[1], fetched_data[2], fetched_data[3])\n",
    "            # write from parsed_data to csv\n",
    "            row_to_write = [parsed_data[key_to_write] for key_to_write in headers_in_order]\n",
    "            writer.writerow(row_to_write)\n",
    "            break # only 1 result\n",
    "        \n",
    "print (\"Output file:\", output_filename)    \n",
    "db_cursor.close()\n",
    "db_conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
